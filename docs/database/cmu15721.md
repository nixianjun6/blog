# CMU15721

## How Query Engines Work

<center>
  ![logicalPlan](../img/logicalPlan.jpg)
  <br>
</center>

<center>
  ![physicalPlan](../img/physicalPlan.jpg)
  <br>
</center>

生成的Logical Plan首先经过Query Optimizer优化。主要介绍Rule-Based Optimizations:

- Projection push-down:尽快过滤掉列。找到projection, filter, aggregate中需要用到的列。Scan时过滤。
- Predicate push-down:尽快过滤掉行。将fiter在join之前做。
- Eliminate Common Subexpressions
- Converting Correlated Subqueries to Joins

经过优化后通过Query Planner转换为Physical Plan。

并行查询执行：

- Combining Results: Map Reduce
- Smarter Partitioning: 负载均衡需要考虑。可以把文件放在目录中，并使用kv对组成的目录名称指定内容。这样可以更早的实现谓词下推，这种方法又叫做"partition pruning".

分布式查询执行：

- Joins：必须首先在连接键上对两个表重新分区，并把分区写入磁盘。
- 需要一个查询调度程序：
  
  生成分布式的Query Plan
  
  序列化Query Plan
  
  序列化数据
  
  选择通信协议
  
  优化

------

## In Memory Database

- **Staring into the Abyss: An Evaluation of Concurrency Control with One Thousand Cores**

**Motivation:** 多核时代到来，由于数百个线程并行运行，协调竞争数据访问的复杂性将成为可扩展性的主要瓶颈，并且可能会减少增加核心数量带来的收益。这篇文章在主存数据库上实现了七种并发控制算法，并在1000核的CPU模拟器上对这些算法进行评估多核可扩展性。

**并发控制算法：**

- 2PL with Deadlock Detection (DL_DETECT)
- 2PL with Non-waiting Deadlock Prevention (NO_WAIT)：锁请求被拒绝时终止事务，预防死锁发生
- 2PL with Waiting Deadlock Prevention (WAIT_DIE)：如果事务比持有锁的事务早，则允许等待，否则中止并重新启动。
- Basic T/O (TIMESTAMP):读写到晚的时间戳就会abort并重新启动，读时会复制一份。
- MVCC：维护版本列表。
- OCC：写操作在私有工作区进行，提交时判断与其他事务有无重叠。有重叠时终止并重新启动，无重叠则更新到数据库中。
- T/O with Partition-level Locking (H-STORE):数据库分区，每个分区分配一个单线程执行引擎。每个事务开始运行时需要获取访问他所有分区的锁，事务请求到达时分配时间戳，添加到目标分区的所有锁获取队列中。如果事务具有队列中最旧的时间戳，则分区的执行引擎在队列中删除事务并分配给该事务分区访问权限。

​	**通用优化：**

- 内存分配：每个线程分配自己的内存池，根据工作负载自动调整池的大小
- Lock Table: 没有表锁，只有行锁
- Mutexs

​	**2PL锁优化：**

- 死锁检测：多线程更新单个waits-for图导致性能瓶颈, partitioning the data structure across cores and making the deadlock detector completely lock-free
- Lock Thrashing: 2PL主要瓶颈，事务需要直到提交才会放锁。
- Wating vs aborting:给每个事务一个等待阈值，超过阈值则重启，以减少Lock Thrashing问题。

​	**TO优化：**

- 时间戳分配：需要保证每个时间戳仅分配给一个事务。
- 分布式验证：在OCC读的最后阶段，事务的读集合需要与之前事务的写集合比较检测冲突。We solve this problem by using per-tuple validation that breaks up this check into smaller operations.
- 本地分区：we allow multi-partition transactions to access tuples at remote partitions directly instead of sending query requests that are executed by the remote partitions’ worker threads. 

​	**Discussion:**

<center>
  ![concurrencybottleneck](../img/concurrencybottleneck.png)
  <br>
</center>

