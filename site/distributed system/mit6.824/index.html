<!doctype html>
<html lang="en">

<head>
        <title>mit6.824 - nixianjun6</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        
        
        

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
        <link rel="stylesheet" href="../../assets/css/dracula-ui.css">
        <link rel="stylesheet" href="../../assets/css/mkdocs.css">

        
            <link  rel="icon" type="image/x-icon" href="../../img/favicon.ico">
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
                <script>hljs.initHighlightingOnLoad();</script>

</head>

<body class="drac-bg-black-secondary drac-text-grey-ternary drac-text drac-scrollbar-purple">

    <main class="d-flex">

        <!-- block sidebar -->
            <nav id="sidebar" class="sidebar drac-bg-black">
    <div class="custom-menu">
        <button type="button" id="sidebarCollapse" class="btn btn-primary">
            <i class="fa fa-bars"></i>
            <span class="sr-only">Menu</span>
        </button>
    </div>

    <div class="p-4">
        
            <img class="logo" src="../../img/nixianjun.png" height="150px">
        

        <div class="drac-text-center">
            
                <span class="drac-text drac-line-height drac-text-white">nixianjun6</span>
            
        </div>

        <div class="drac-box flex-column">
            <ul class="dot-ul">
                <li><div class="dot-li drac-bg-cyan"></div></li>
                <li><div class="dot-li drac-bg-green"></div></li>
                <li><div class="dot-li drac-bg-orange"></div></li>
                <li><div class="dot-li drac-bg-pink"></div></li>
                <li><div class="dot-li drac-bg-purple"></div></li>
                <li><div class="dot-li drac-bg-red"></div></li>
                <li><div class="dot-li drac-bg-yellow"></div></li>
            </ul>
        </div>

        <hr class="drac-divider" />

        <!-- block menu -->
        <ul class="mb-5 drac-list drac-list-none">
            
            <li class="drac-box">
                <a href="../.."
                    class="
                    drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
                    Home
                </a>
            </li>
            <li class="drac-box">
                <a href="../.."
                    class="
                    drac-anchor btn-toggle d-inline-flex align-items-center border-0 drac-text-purple--hover collapsed"
                    data-bs-toggle="collapse" data-bs-target="#database-collapse" aria-expanded="false">
                    Database
                </a>
                <div class="collapse" id="database-collapse">
                    <ul class="mb-5 drac-list drac-list-none">
                            
    <li class="drac-box-ternary">
        <a href="../../database/cmu15445/"
            class="
            drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
            cmu15445
        </a>
    </li>
                            
    <li class="drac-box-ternary">
        <a href="../../database/leveldb/"
            class="
            drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
            leveldb
        </a>
    </li>
                    </ul>
                </div>
            </li>
            <li class="drac-box">
                <a href="../.."
                    class=" active 
                    drac-anchor btn-toggle d-inline-flex align-items-center border-0 drac-text-purple--hover collapsed"
                    data-bs-toggle="collapse" data-bs-target="#distributed-system-collapse" aria-expanded="false">
                    Distributed System
                </a>
                <div class="collapse" id="distributed-system-collapse">
                    <ul class="mb-5 drac-list drac-list-none">
                            
    <li class="drac-box-ternary">
        <a href="./"
            class=" active 
            drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
            mit6.824
        </a>
    </li>
                    </ul>
                </div>
            </li>
            <li class="drac-box">
                <a href="../.."
                    class="
                    drac-anchor btn-toggle d-inline-flex align-items-center border-0 drac-text-purple--hover collapsed"
                    data-bs-toggle="collapse" data-bs-target="#algorithm-collapse" aria-expanded="false">
                    Algorithm
                </a>
                <div class="collapse" id="algorithm-collapse">
                    <ul class="mb-5 drac-list drac-list-none">
                            
    <li class="drac-box-ternary">
        <a href="../../algorithm/algorithm/"
            class="
            drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
            algorithm
        </a>
    </li>
                    </ul>
                </div>
            </li>
            <li class="drac-box">
                <a href="../.."
                    class="
                    drac-anchor btn-toggle d-inline-flex align-items-center border-0 drac-text-purple--hover collapsed"
                    data-bs-toggle="collapse" data-bs-target="#language-collapse" aria-expanded="false">
                    Language
                </a>
                <div class="collapse" id="language-collapse">
                    <ul class="mb-5 drac-list drac-list-none">
                            
    <li class="drac-box-ternary">
        <a href="../../language/C%2B%2B/"
            class="
            drac-anchor d-inline-flex align-items-center border-0 drac-text-purple--hover">
            language
        </a>
    </li>
                    </ul>
                </div>
            </li>
        </ul>
        <!-- endblock -->
    </div>
</nav>
        <!-- endblock -->

        <nav class="divider drac-bg-purple-cyan"></nav>

        <div class="content">
            <!-- block header -->
                <header>
    <nav class="navbar navbar-expand-xl drac-bg-purple"">
        <div class="container-fluid">

            <!-- <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsMenu"
                aria-controls="navbarsMenu" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button> -->

            <div class="collapse navbar-collapse flex-column ml-auto" id="navbarsMenu">
                <ul class="navbar-nav">

                    <!-- block preview -->
                    <li class="nav-item">
                            
        <div class="container">
            <div class="row row-preview">
                <div class="col">
                    <a href="../../database/leveldb/"
                        class="btn-preview drac-btn drac-btn-outline drac-text-white drac-text-cyan-green--hover">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </div>
                <div class="col">
                    <a href="../../algorithm/algorithm/"
                        class="btn-preview drac-btn drac-btn-outline drac-text-white drac-text-cyan-green--hover" style="padding-left: 3%;">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </div>
            </div>
        </div>
                    </li>
                    <!--  endblock -->

                    <!-- block search -->
                    <li class="nav-item"><div role="search" class="search-box">
	<form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
		<input type="text" name="q" class="drac-input drac-input-search drac-input-white drac-text-white drac-bg-black-secondary"
		placeholder="Search docs" title="Type search term here" />
	</form>
</div>
                    </li>
                    <!--  endblock -->

                    <!-- block source -->
                    <li class="nav-item">
                        
                    </li>
                    <!--  endblock -->

                </ul>
            </div>

        </div>
    </nav>
</header>
            <!-- endblock -->

            <!-- block content -->
                <section class="p-md-5 section-content">
    <article>
        <p><h1 id="mit6824">MIT6.824</h1>
<p>​   人们想从分布式系统中获得许多好处，比如通过并行的方式提高性能，通过复制副本的方式增加容错。但分布式系统的构建也面临很多挑战，比如更多的机器意味着故障的频率发生更高，复制副本会产生各副本之间一致性的问题等。这门课主要探究的主题包括：</p>
<ul>
<li>
<p>容错：分布式系统能够自动的纠正一些错误（比如部分服务器宕机，网络延迟故障），并且具备一定的恢复故障的能力。这通常是通过在多个服务器上存储多个副本实现的。</p>
</li>
<li>
<p>一致性：当我们使用多个副本来保证系统的容错能力时，我们需要进一步考虑各副本数据之间一致性的问题。</p>
</li>
<li>
<p>性能：我们希望通过N台服务器来获得N倍的CPU并行效率，N倍的磁盘空间。但这可能会导致负载不均衡等问题。同时，性能和以上两点是矛盾的，需要根据具体业务进行合理的权衡。</p>
</li>
</ul>
<hr />
<h2 id="mapreduce">Mapreduce</h2>
<p>​       首先通过一个例子初步感受一下分布式系统的计算过程：Mapreduce。</p>
<p><center>
  <img alt="mapreduce" src="../../img/mapreduce.png" />
    <div>mapreduce execution overview</div>
</center></p>
<p>​       Mapreduce的设计初衷是为了方便不熟悉分布式系统的程序员更好的处理数TB数据量的计算任务。它将一类的任务抽象为了Map + Reduce两个阶段。其中Map的过程类似于以下代码：</p>
<pre><code class="language-Go">map(String key, String value):
// key: document name
// value: document contents
for each word w in value:
EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<p>​       Reduce过程类似于以下代码：</p>
<pre><code>reduce(String key, Iterator values):
// key: a word
// values: a list of counts
int result = 0;
for each v in values:
result += ParseInt(v);
Emit(AsString(result));
</code></pre>
<p>​       举个具体的例子：词频统计任务。词频统计任务是指统计数据集中每个单词出现的次数。</p>
<ul>
<li>
<p>首先系统会将整个数据集进行split操作，划分成了数个文件。</p>
</li>
<li>
<p>之后系统会对每个文件进行Map操作。对于词频统计任务来说，这里的Map操作就是遍历文件中的每个单词w，生成一个(w,1)这样的map。这样我们得到了一个类似于[(w1, 1), (w2, 1), (w3, 1), ...]的map list输出。</p>
</li>
<li>
<p>Map阶段结束之后，往往会有一个shuffle的阶段，并写入到本地磁盘中，以方便后续Reduce阶段操作。在词频统计任务中，我们可以对map list中每个map的key进行哈希操作。这样根据哈希值不同，这个map list可能会被划分成map list 1, map list 2, ...我们把每一个map list i(i = 1, 2,  ...)存储到本地的一个文件中。</p>
</li>
<li>
<p>当所有的文件都完成Map操作之后，我们再将所有哈希值相同的map list i进行Reduce操作，比如在词频统计任务中，我们将相同哈希值的map list i 进行合并并按key进行排序。之后我们将具有相同key的map的value求和得到一个新的map list, 并写入到磁盘中，就能得到完成词频统计的任务。</p>
</li>
</ul>
<p>​       当不熟悉分布式的程序员处理类似任务时，他只需要将任务拆分成Map任务+Reduce任务并定义好Map函数和Reduce函数传入系统，系统就会帮助他以分布式的形式完成这个任务。具体来说，MapReduce中存在两种类型的机器: Master和worker。其中Master负责将map和reduce任务分配给worker，而worker负责具体执行Map和Reduce任务。</p>
<p>​   这样设计的系统可能需要考虑以下问题:</p>
<ul>
<li>
<p>Master和worker的协作方式: 采用RPC的方式实现。这样的方式的特点是当机器A上的进程想要运行一个函数f(x,y)，机器A并不使用自身的资源去运行函数f(x,y)，而是将函数名，传入参数的引用以及返回值的引用传递给机器A上的stub，通过网络，机器A上的stub将这些消息传递给了机器B上的stub，机器B上的stub会调用本地的方法得到返回值，并把返回值传递回机器A上的stub，进一步由机器A上的stub返回给机器A上的进程。</p>
</li>
<li>
<p>保证写文件的过程是原子的：一个小trick，可以先写到临时文件中，写操作完成后通过重命名完成写文件的原子性。</p>
</li>
<li>
<p>worker宕机处理：一般的做法是Master启动一个后台程序定时检查已分配任务的状态，如果任务分配之后指定时间内没有完成，需要将该任务进行重新分配。更好的做法是当worker询问新任务时，如果发现所有任务都已经分配的情况下进行检查。这样的做法一定是正确的，因为即使Master定期检查到部分任务出现故障，也要等worker询问新任务才会重新分配出去。</p>
</li>
</ul>
<p>​   除此之外，我个人还有以下思考问题：</p>
<ul>
<li>
<p><strong>具体来说，什么样的任务可以分解为Map + Reduce的形式?还有其他适用于分布式的分解形式嘛?</strong></p>
</li>
<li>
<p><strong>Master的容错应该如何保证?</strong></p>
</li>
<li>
<p><strong>如果Reduce过程不要求一定要在Map过程全部完成后就开始，需要考虑哪些细节?</strong></p>
</li>
</ul>
<p>​   </p>
<hr />
<h2 id="gfs">GFS</h2>
<p>​   以下内容来自于<a href="https://www.bilibili.com/video/BV1fT411c7y6?vd_source=97ac3eaed2a0c7a9d639b4a2cb7028a9">戎米的论文笔记</a></p>
<p>​   文件系统是用于存储文件，并进行读取等操作的系统。分布式系统需要依赖底层的文件系统管理所有需要持久化的文件。GFS是最著名的分布式文件系统，是分布式系统正式开始大规模商用的奠基之作。</p>
<p><center>
  <img alt="GFS" src="../../img/GFS.png" />
    <div>Evolution of the file system</div>
</center></p>
<p>​   如图所示，GFS相比于单机文件系统，存在以下新的问题:</p>
<ul>
<li>在多台机器上文件的扩缩容</li>
<li>多台机器上读取文件</li>
<li>服务器故障文件不损坏不丢失</li>
<li>复制样本的一致性</li>
<li>大文件存储</li>
<li>自动监控，容错，恢复</li>
<li>快速的顺序读和追加写</li>
</ul>
<p><center>
  <img alt="GFS Architecture" src="../../img/GFS_Architecture.png" />
    <div>GFS Architecture</div>
</center></p>
<p>​   </p>
<p>​       如图所示，GFS中会存在三种节点:client, master, chunkserver。其中client负责维持专用接口，和应用直接交互。master负责维持文件的元数据， 管理文件的存储以及租约。chunkserver用于存储数据。</p>
<p>​   文件存储: GFS会把文件切分成一个一个的chunk进行存储，这样地址不连续的存储很方便文件的扩缩容，同时也方便各chunkserver的均衡。同时考虑到GFS需要支持大文件存储，因此chunk的大小设置为64MB这样一个偏大的值。这样做有以下好处:</p>
<ul>
<li>
<p>可以有效减少系统内部的寻址和交互次数。</p>
</li>
<li>
<p>除此之外，大的chunk以为着client可能在一个chunk上执行多次操作，这样可以复用TCP连接，节省网络开销（也会带来热点问题，牺牲掉了部分性能）。</p>
</li>
<li>
<p>当然，更大的chunk意味着更少的chunk数量。这对GFS非常重要，因为master只有一台机器，更少的chunk数量节省了元数据的存储开销，相当于节省了系统内最珍贵的内存资源。</p>
</li>
</ul>
<p>​       Master: Master主要维护两张表。一张是文件名与chunk id的映射关系，以及chunk id和chunkserver的映射关系。实际上，我们只需要对第一张表进行持久化即可，第二张表的信息可以在master开机之后从各个chunkserver处重新收集。因此，通过Master我们就能准确找到我们需要读取的文件的位置。由于GFS采用单Master的设计（实现难度低，强一致性），因此GFS需要一系列的设计来确保master不会成为整个系统的瓶颈:</p>
<ul>
<li>
<p>数据流都不经过master，而是由client直接与chunkserver交互</p>
</li>
<li>
<p>client会缓存master的元数据</p>
</li>
<li>
<p>增大chunk大小，压缩元数据</p>
</li>
</ul>
<p>​   <strong>Master的高可用设计:</strong></p>
<ul>
<li>
<p>持久化文件名和chunk id的映射关系</p>
</li>
<li>
<p>shadow master作为备份</p>
</li>
<li>
<p>所有的写数据之前都要记录日志(WAL)</p>
</li>
<li>
<p>向shadow master同步WAL</p>
</li>
</ul>
<p>​       如果发生Master宕机，通过Chubby进行切换shadow master</p>
<p>​   <strong>chunkserver的高可用设计:</strong></p>
<ul>
<li>
<p>每个chunk会有三个副本，存储在不同的chunkserver中</p>
</li>
<li>
<p>对一个chunk的写入必须保证三个副本都已写入才算写入成功，否则会重复进行一次相同的写入</p>
</li>
<li>
<p>如果有chunkserver宕机，master会在另外的chunkserver上重建副本，将副本数目维持在3个</p>
</li>
<li>
<p>读取数据时，需要通过校验和进行检查。</p>
</li>
<li>
<p>Master会下放租约给某个chunk副本，租约生效期间，对这个chunk的读写由该副本负责，该副本称为primary。租约的有效期为60s，如果发现primary宕机了，则会在租约到期后重新指定primary（防止split brain）。</p>
</li>
<li>
<p>Master会选择资源利用率较低，最近创建活动较少，与其他副本不在同一机架上的chunkserver创建新副本。</p>
</li>
</ul>
<p>​           <strong>GFS读写流程:</strong></p>
<p>​           写入过程: Client和Master进行交互获取文件位置，之后会按距离的拓扑顺序将数据推送到各个副本中。确保所有的数据推送完成后，会向Primary发起写入请求，Primary会按一定的顺序将数据写入，并让其他副本也按照这一顺序进行写入。如果所有的副本都写入成功，则通知Client写入完成，否则会让Client重新推送一次数据，进行后续同样的操作。</p>
<p><center>
  <img alt="GFS Write Flow" src="../../img/GFS_write_flow.png" />
    <div>GFS Write Flow</div>
</center></p>
<p>​           如果涉及多个chunk，则分多次进行以上操作，确保每次操作仅写入一个chunk。</p>
<p>​       读取流程: Client首先查看cache中有无文件的元数据，如果没有则和Master交互获取文件位置，然后client和离自身最近的副本发送读请求，如果通过校验和确认则读取完成，否则读取其他副本，直到通过校验和。</p>
<p>​   GFS的一致性模型:</p>
<p><center>
  <img alt="GFS Consistency Model" src="../../img/GFS_consistency_model.png" />
    <div>GFS Consistency Model</div>
</center></p>
<p>​   这里解释一下defined 与consistent的差别在于，defined保证读取最新的写入是一致的。而consistent不保证最新写入的数据一致。</p>
<hr />
<h2 id="vmware-ft">VMware FT</h2>
<p>​   背景：当我们构建一个服务时，计算机硬件，网络都有可能发生故障。但是我们仍然希望能够稳定的提供服务。一个可行的办法是：复制。</p>
<p>​   复制的限制：复制能够解决单台计算机的fail-stop故障。fail-stop故障是指当计算机出现故障时，那么它会单纯的停止运行。但是复制无法解决软件中的bug和硬件设计中的缺陷。并且复制的副本之间的错误需要是相互独立的，如果他们之间的错误是有关联的，那么复制对我们就没有帮助。除此之外，复制需要更高昂的成本，这往往取决于单个副本失效会对你造成多大的损失和不便。</p>
<p>​   常见的复制方法包括两种: State Transfer和Replicated State Machine。State Transfer背后的思想是Primary定期将自己完整的状态，比如内存中的内容，拷贝并发送给Backup。Backup会保存收到的最后一次的状态。复制状态机则假定某一状态在接受到相同的外部输入时会转换到确定的下一个状态。因此，Primary只需要定期将外部事件发送给Backup，Primary和Backup就能保持相同的状态。State Transfer的好处在于不需要更多的假设，更为简单的保证了副本之间的一致性。而复制状态机的好处在于，外部事件往往比服务的状态要小，可以提升复制的性能。</p>
<p>​   复制状态机设计的主要问题：</p>
<ul>
<li>状态的定义</li>
<li>Primary与Backup之间的同步频率</li>
<li>Primary发生故障时的切换方案</li>
<li>当有副本发生故障时，如何维持我们需要的副本数量</li>
</ul>
<p>​   状态定义：VMware FT会复制机器的完整状态，这包括了所有的内存，寄存器等所有信息。优点在于对于VMware FT管理的机器上的所有软件都具备了容错性。缺点是，它没有那么的高效。</p>
<p>​   切换方案：</p>
<ul>
<li>
<p>Primary宕机:当Backup一段时间没有收到Primary的消息时，Backup将会Go alive，不再受来自于Primary的事件驱动，而是自由执行。Backup会在网络中发送消息，让后续的客户端请求发往Backup，而不是Primary。</p>
</li>
<li>
<p>Backup宕机:Primary如果一段时间没有收到Backup的消息，则会抛弃Backup停止向它发送事件。</p>
</li>
<li>
<p>避免重复输出：这里可以通过TCP意外的解决异常的场景。</p>
</li>
</ul>
<p>​   非确定性事件：当一个状态在接受到相同的外部输入时，有可能会转换到不同的状态中。包括网络数据包产生中断的时间，奇怪的指令（包括随机数生成，获取当前时间，获取计算机MAC地址等），多CPU的并发。非确定性时间只能通过状态转移的方法保持一致性。</p>
<p>​   输出控制：Primary必须保证Backup的确认收到了Log,才能对客户端进行回复。（对性能造成了很严重的限制。有趣的几个想法：Primary指令执行和与Backup通信同时进行；输入送到Primary,输出从Backup送出）</p>
<p>​   Test-and-Set：为了解决因为网络故障而导致的脑裂，需要一个第三方权威机构对Backup的上线进行认证。</p>
<hr />
<h2 id="raft">RAFT</h2>
<p>​   背景：在我们目前了解的Mapreduce, GFS, VMware FT三个系统中，都有着一个共同的特性，那就是他们需要由一个单节点来决定在多个副本中的Primary。使用单个节点可以让问题更加简单（比如不会脑裂），但缺点是它本身又是一个单点故障。RAFT是共识算法的一种，使用共识算法的目的在于实现复制状态机，保证多副本命令序列的一致，从而保持他们的状态一致。RAFT通过过半票决(Majority vote)的方式避免了单节点决策以及脑裂的问题，同时能够自动完成故障情况下的切换。这里过半票决指的是，系统在任何时候为了完成任何操作，都必须凑够过半的服务器来批准相应的操作。</p>
<p>​   <strong>选主：</strong>在多副本系统中，我们需要一个主副本来应答客户端的请求。在之前的系统中，这往往是由一个单点控制的。而在RAFT中，这是由整个RAFT节点集群决定的。如下图所示，当系统开机时，所有的RAFT节点都处于一个follower的状态，可以认为他们都是备选副本。当一个节点长时间没有收到leader（保存主副本的节点）的消息时，他们会发起选举，推任自己为candidate，并向其他节点请求选票。如果他们的请求获取了大部分节点的同意，那么它将成为leader。否则如果有其他节点当选，那么它会收到Leader的消息，它会变回follower。如果没有其他节点当选，那么它会再次等待一段时间，并将重新发起选举。</p>
<p><center>
  <img alt="raft_state" src="../../img/raft_state.png" />
    <div>RAFT STATES</div>
</center></p>
<p>​       关于这一部分，我们可能会有以下问题：</p>
<ul>
<li><strong>如何避免脑裂？</strong></li>
</ul>
<p>​       RAFT中每个节点都保存了任期信息currentTerm，这个信息从0开始，并且单调递增。每当一个节点发起一次选举，他会认为当前任期的leader已经失效，那么他会自增currentTerm，去竞选下一个任期的leader。当多个condidate同时竞选leader时，其他节点会选出任期最高的节点做leader，如果任期最高的节点不止一个，则会按收到请求投票的顺序投票给时间最早的candidate，并且会更新自身的任期与任期最高的节点保持一致。同时，如果candidate知道有任期比自己高的节点存在，那么candidate就会转变为follower,并且同步最新的任期。这就保证了<strong>只有最高任期内拥有过半投票的candidate才会竞选成为leader</strong>，从而避免了脑裂问题。</p>
<ul>
<li><strong>如何避免活锁？</strong></li>
</ul>
<p>​       RAFT选主过程还会面临活锁的问题，特别是在网络不可信赖的情况下。第一个设计在于，follower不是在固定时间内没收到leader消息而发起选举的。这个超时时间往往是一个区间内的随机数，论文中推荐的时间是150-300ms（实际上不一定）。但这还不够，我们看个例子：在一个三个节点的RAFT系统中，0号节点发起超时选举，1号节点投票给了0号。在0号节点当选leader并发送心跳信息给1号之前，1号节点也发起了超时选举。之后0号成为leader，发送心跳消息之前，收到了1号的投票请求并投票给了1号，0号又变为了follower。在下次1号发送心跳消息之前，0号又发起超时选举，这样周而复始就陷入了活锁的情况。解决这种情况，我认为有两个必要的设计，一个在于<strong>broadcastTime &lt;&lt; electionTimeout</strong> 。其中广播时间是指节点间通信的时间。需要考虑到其中部分节点已经完全通信不了，因此选举请求投票时应该保证一个最长通信时间，这需要远小于超时选举时间。亦或者不需要等所有节点完成投票，只要选票数超过一半则立刻转换为leader并开始发送心跳。第二个设计在于，<strong>当follower完成投票后也重置选举超时时间</strong>，不需要等到leader发送心跳。这个设计我不确定是否一定有必要，但是在保证正确性的情况下也是一种直觉上更优的设计。</p>
<p>​   <strong>日志复制：</strong>当客户端有新的请求到来时，leader会将请求存储在日志中，并与其他节点进行同步，只有当超过半数的节点复制好日志后才会提交到本地应用程序中。这里可能会有三个问题：</p>
<ul>
<li>
<p>如果只有新的请求到来时，才发起日志复制。那如果上一任leader写入了新的日志，并且他复制给了部分节点（没有超过半数），之后它宕机了。这些被复制了新日志的节点当选了leader，那这条新日志不是也应该立刻与其他节点同步吗？事实上，由于没有提交到本地应用程序中，因此这并不会造成什么错误，但这可能也是一个可以优化的问题。</p>
</li>
<li>
<p>如果超过半数节点完成复制，leader提交之后宕机了。而其他节点并没有完成提交，这时切换到其他副本时，就会发现上层应用程序的状态不一致。这个问题RAFT论文中并没有给出解决的办法。我个人认为的可能的解决方式是，根据RAFT安全性章节对于选举条件的进一步限制，一定是这些复制了新日志的节点当选leader，那么如果一个新节点当选leader时，往自己的日志中塞入一条空日志，那么就会进一步引发日志复制保证了这次提交了。同时也解决了我上一个提到的问题。</p>
</li>
<li>
<p><strong>如何保证主副本之间的一致性。</strong>RAFT通过一种非常保守的方式进行一致性检查。leader首先找到follower与自己日志相同的部分，第一次找的过程通常会从leader日志最后的位置开始找，如果不匹配则往前一个位置继续去进行匹配。匹配上了之后，会比较follower日志的后部分与leader想要同步的新日志（当网络不可靠时，可能会有一些老日志想要发起同步，这时不能直接截断），当出现第一个冲突的地方时，直接截断follower后部分日志，并把新日志写入follower中。之后leader就会从与该follower最新一次同步的日志的最后位置开始去匹配。这个过程首先听上去可以优化的点在于匹配的过程，一个一个往前找在某些情况下可能会有很大的性能损失。但我个人觉得，加入日志快照功能之后，这里的优化必要性不大。另一个问题在于，日志不能乱序存放，这当然会损失掉一些性能。但是RAFT本身的目的在于实现一个简单易懂的共识算法，因此这里选择了非常保守的策略。</p>
</li>
</ul>
<p>​   <strong>安全性：</strong>安全性主要是对选主和日志复制过程中的corner case进行修正。首先是选举限制，即Leader不仅需要是最高任期同时还需要拥有最新的日志。这里最新的日志是指，最后一条日志的任期是所有副本所有日志中最大的，如果有相同的任期的其他日志，那么这条日志存储的位置的索引是最大的（日志是按顺序存储的）。这主要是防止某台机器宕机很久，然后重新连入集群中，变为leader将其他节点的日志都覆盖掉。然后是对日志提交提出了新的要求，提交的最后一个日志不仅需要大多数节点同意，同时必须是当前任期的。否则就会出现下图的情况：</p>
<p><center>
  <img alt="raft_figure8" src="../../img/raft_figure8.png" />
    <div>RAFT FIGURE8</div>
</center></p>
<p>​   <strong>集群成员变更：</strong>待补充...</p>
<hr />
<h2 id="linearizability">Linearizability</h2>
<p>以下内容参考了<a href="https://zhuanlan.zhihu.com/p/42239873">线性一致性</a>与<a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-08-zookeeper/8.3-xian-xing-yi-zhi-linearizability3">mit6.824翻译</a></p>
<p>​   问题的提出：当一个多副本的服务或任意其他的服务正确运行意味着什么？</p>
<p>​   我们对于正确的定义就是Linearizability 线性一致或者说Strong consistency强一致。直觉上来说，一个服务是线性一致的，那么它的表现的就像只有一个服务器，并且服务器没有故障，这个服务器每次执行一个客户端请求。更准确地说，一个系统的执行历史是一系列的客户端请求，如果执行历史整体可以按照一个顺序排列，且排列顺序与客户端请求的实际时间相符合，那么它就是线性一致的。</p>
<p>​   对于客户端的一系列请求op1, op2, op3, ... , 对应的请求有一个客户端发出的时间send1, send2, send3, ... , 对应的服务端的回复时间reply1, reply2, reply3, ...。要达到线性一致，我们需要确定请求的排列顺序。这样的排列顺序需要满足:</p>
<ul>
<li>
<p>如果reply i &gt; send j, 那么我们连一条i 到j 的有向边，表示请求i 在请求j 之前。</p>
</li>
<li>
<p>如果 op j 为读， 并且它读到了key k对应的value v， 那么一定存在一个请求op i为写，写入内容是将key k对应的value修改为v。我们连一条i到j的有向边，表示请求i在请求j之前。</p>
</li>
</ul>
<p>​   几个不是线性一致的例子：</p>
<p><center>
  <img alt="linearizability1" src="../../img/linearizability1.png" />
    <div>线性不一致 Ex1</div>
</center></p>
<p>​   这个例子给我们的教训是：对于系统执行写请求，只能有一个顺序，所有客户端读到的数据的顺序，必须与系统执行写请求的顺序一致。</p>
<p><center>
  <img alt="linearizability2" src="../../img/linearizability2.png" />
    <div>线性不一致 Ex2</div>
</center></p>
<p>​   这个例子给我们的教训是：对于读请求不允许返回旧的数据，只能返回最新的数据。</p>
<p>​   下面是Lab3的kv raft的架构图：</p>
<p><center>
  <img alt="raft_diagram" src="../../img/raft_diagram.png" />
    <div>raft diagram</div>
</center></p>
<p>这里我们主要考虑在保证线性一致的情况下一些可能的优化：</p>
<p>​   首先，Get请求不需要传入Raft层进行共识，因为Get请求并不会改变状态。但是需要额外的操作，否则当出现网络分区的情况时，会发生线性不一致的情况。</p>
<ul>
<li>
<p>ReadIndex: 首先，leader必须返回最新的committed index，因此在leader当选时需要提交一个空日志（所以上一节我在日志复制章节里的想法是正确的），然后再进行read。其次读操作时，需要发送心跳确认自己的leader身份（防止网络分区）。</p>
</li>
<li>
<p>lease read: 通过lease机制维护Leader的状态，减少ReadIndex每次read发送心跳的开销（这里必须保证租约的时间小于选举超时时间，这样租约期间只有一个leader）。实际上还能甚至不必等到ReadIndex apply，服务端可以直接返回读请求，这样也是满足线性一致性的。但同时需要注意，由于新Leader可能落后于老Leader，因此只有在Leader的状态机应用了当前term的第一个Log之后才能LeaseRead。</p>
</li>
<li>
<p>follower read：先去 Leader 查询最新的 committed index，然后拿着 committed Index 去 Follower read，从而保证能从 Follower 中读到最新的数据。这个优化明显是最好的，因为他将leader的部分工作负载分担给了follower。</p>
</li>
</ul>
<hr />
<h2 id="zookeeper">Zookeeper</h2>
<p>​   问题的提出：Raft实际上是一个库，并不能直接提供可以交互的独立的服务，同时我们也知道要实现一个线性一致的基于RAFT的服务还需要花费额外的努力。因此，Zookeeper旨在能够提供一个通用的协调服务帮助构建分布式应用。</p>
<p>​   一致性保证：我们知道通过RAFT + follower read可以实现线性一致性。但同时在Read的过程中，还是需要花费一次和Leader通信的开销。Zookeeper为了减少这样的开销，而放弃了线性一致性。即Zookeeper的客户端会直接找follower去进行数据读取。Zookeeper这里的一致性保证是写请求（包括即读又写的请求）线性一致（写入raft层log里肯定是线性一致的）和FIFO client order（即对于一个客户端来说，会按照客户端发送的顺序来执行）。由于会出现follower副本不可用的情况，因此客户端会记录最后的读请求对应的applyIndex。对于后续的读请求必须不晚于这个applyIndex。（这里的applyIndex是我的理解）。</p>
<p>​   线性一致性：如果想要做到线性一致性，可以使用sync操作，原理是在读操作之前发送一个空的写请求，以保证读操作能读到最新的数据。（本质上我的理解就相当于把读操作写入log了，估计现在的版本已经优化了）。</p>
<p>​   更具体的来说，Zookeeper可以解决大的数据中心单点失效的问题，比如MapReduce,  GFS中的Master， VMware FT中的TEST-AND-SET。这是Zookeeper设计API时一些具体的动机。同时，为了让一个Zookeeper集群能够运行多个服务，Zookeeper将API设计成了一个层级化的目录结构。这些目录和文件被称为znodes。</p>
<p>​   znodes: Zookper设计了三种znodes。包括Regular znodes，这种znode一旦创建，就永远存在，除非主动删除。第二种叫Ephemeral znodes，如果Zookeeper认为创建它的客户端挂了，它会删除这种类型的znodes。最后一种叫Sequential znodes，当多个客户端同时想以特定名字创建文件时，会在文件名之后加一个数字，并且保证数字不重合，且一定是递增的。</p>
<p>​   API: Zookper以RPC的方式暴露以下API:</p>
<ul>
<li>CREATE(PATH, DATA, FLAG)：PATH为文件路径，DATA为数据，FLAG为znode类型。当存在该文件时，会返回FASLE，否则会创建文件，返回TRUE。</li>
<li>DELETE(PATH, VERSION): VERSION为版本号，每个znode都会有一个版本号，当znode有更新时，version也会随之增加。表示要删除某个版本的文件。</li>
<li>EXIST(PATH, WATCH)：WATCH可以监听文件的变化，当文件有任何变更都会通知客户端。表示某个文件是否存在。</li>
<li>GETDATA(PATH, WATCH)。</li>
<li>SETDATA(PATH, DATA, VERSION)</li>
<li>LIST(PATH)。返回路径下所有文件。</li>
</ul>
<p>​   一些应用(Zookeeper只能保证一些简单事务的原子性，称之为mini-transaction)：</p>
<p>​   计数器：</p>
<pre><code>WHILE TRUE:
    X, V = GETDATA(&quot;F&quot;)
    IF SETDATA(&quot;f&quot;, X + 1, V):
        BREAK
</code></pre>
<p>​       Non-Scalable Lock：</p>
<pre><code>WHILE TRUE:
    IF CREATE(&quot;f&quot;, data, ephemeral=TRUE): RETURN
    IF EXIST(&quot;f&quot;, watch=TRUE):
        WAIT
</code></pre>
<p>​       这种锁和上一个计数器都有羊群效应，复杂度为O(n^2)</p>
<p>​       Scalable Lock:</p>
<pre><code>CREATE(&quot;f&quot;, data, sequential=TRUE, ephemeral=TRUE)
WHILE TRUE:
    LIST(&quot;f*&quot;)
    IF NO LOWER #FILE: RETURN
    IF EXIST(NEXT LOWER #FILE, watch=TRUE):
        WAIT
</code></pre>
<hr />
<h2 id="craq">CRAQ</h2>
<p>​   背景：CRAQ是对于CR的一种旧方案的改进，但是CRAQ实现了将读请求分发到任意副本执行并能够保证线性一致性。</p>
<p>​   CR：在CR系统中，服务器按链排列，第一个服务器称为HEAD，最后一个称为TAIL。当客户端发送一个写请求，写请求总是先送到HEAD，HEAD会先在本地apply之后再将写请求通过链传向下一个服务器，以此类推，直到写请求传到TAIL，TAIL apply之后会回复客户端。对于读请求，客户端会发往TAIL。当没有故障的时候，整个系统就像只有TAIL一台机器一样 ，因此CR是线性一致的。当某个节点出现故障时，基本上只要将故障节点从链中移除即可。</p>
<p>​   Why CR: CR相比于RAFT来说，拥有更好的性能，这时因为当客户端请求变多时，RAFT leader很容易会成为bottle neck，而CR则会有HEAD和TAIL分担请求并且HEAD只用将请求发送给下一个节点而不是其他所有节点，因此bottle neck会来的更晚一些。除此之外，CR的故障恢复也更容易。</p>
<p>​   CR drawback: CR无法处理网络分区的问题，当HEAD和第二个节点的网络连接出现问题时，会出现脑裂现象。因此CR并不是一个完整的复制方案，需要一个第三方的权威机构Configuration Manager去决定一条链的节点组成。CRAQ基于Zookeeper构建了Configuration Manager。（实际上还是不能解决HEAD和第二个节点无法通信的问题，需要更多的细节来设计Configuration Manager）</p>
<p>​   CRAQ进一步构建一个更高效的系统：实现数据分片，每个分片可以是一条链(不一定是链，使用CR或是RAFT还是其他的共识算法，取决于你的场景。比如RAFT会受到Leader的限制，而CR也会受到链上最慢的服务器的限制)，每个链都可以构建极其高效的结构存储数据，进而进一步加大并发的读写请求。同时，也不会出现网络分区问题，因为这些链会由一个可靠的，不会脑裂的Configuration Manager管理。</p>
<hr />
<h2 id="aurora">Aurora</h2>
<p>​   以下内容很多引用了<a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-10-cloud-replicated-db-aurora/10.1-aurora-bei-jing-li-shi">mit6.824翻译</a></p>
<p>​   背景：最开始Amazon提供的云产品叫EC2。Amazon在每个服务器上都会运行VMM，并且搭载许多个EC2实例。每个EC2实例上都会运行一个标准的操作系统，在操作系统之上运行各种应用程序，比如web服务，数据库等。这些服务当中，数据库相比于其他服务，需要持久化存储数据。一个简单的方式是持久化存储数据存储在服务器的本地硬盘上，但这样会出现单点失效的问题。而实际上Amazon本身有块存储的服务，叫做S3。这样可以定期对数据库做snapshot，并将snapshot存储在S3上，并基于snapshot做故障恢复。这样做的问题在于，会损失两次snapshot之间的数据。所以Amazon又提供了EBS服务以实现容错且支持持久化存储的服务。EBS底层是通过CR构建的双副本系统。尽管EBS是一次很大的进步，但仍然存在一些问题：大量的网络负载与较低的容错性（出于性能考虑，EBS双副本存储在一个数据中心）。</p>
<p><center>
  <img alt="RDS" src="../../img/RDS.png" />
    <div>RDS</div>
</center></p>
<p>​   于是Amazon又提供了RDS服务加强了容错性。尽管这种架构增强了容错性，因为在不同的数据中心有了第二个副本拷贝，但是RDS的性能十分糟糕。于是，Amazon为了兼顾性能和容错，放弃了这样通用的存储服务，提出了Aurora。</p>
<p><center>
  <img alt="Aurora_firstlook" src="../../img/Aurora_firstlook.png" />
    <div>first look at Aurora</div>
</center></p>
<p>​   Aurora的改变：Aurora将原来的EBS替换成了分布在3个数据中心的6个副本。同时，网络通信传递的数据只有Log条目（Log相对于整个data page来说小很多，但这里的存储系统无法理解Log，因此不再是通用的存储服务，而是应用定制的存储系统）。另一个重要的事情是，不再需要所有副本都确认才能继续执行操作，而只需要的Quorum形成就可以继续执行操作了。</p>
<p>​   Quorum Replication：假设有N个副本。为了能够执行写请求，必须要确保写操作被W个副本确认，W小于N。所以你需要将写请求发送到这W个副本。如果要执行读请求，那么至少需要从R个副本得到所读取的信息。这里的W对应的数字称为Write Quorum，R对应的数字称为Read Quorum。这是一个典型的Quorum配置。必须满足R + W &gt;= N + 1，这样至少确保一个副本即收到了最新的写请求，也收到了最新的读请求。这样，读请求由版本号最高的副本进行回复就可以读到最新的数据。相比于CR，QR的优势在于可以容忍暂时的慢副本，以及能够更灵活的调整系统的读写性能。Aurora的取值为N=6,W=4,R=3。</p>
<p>​   Why need QR Read:实际上每次QR Write之后，每个存储服务器都会告知数据库服务服务器他们确认的log index。因此只要每次向拥有最高连续log index的服务器发起读请求即可，而不需要QR Read。但是，当数据库服务器宕机时，Amazon会自动启动一个新的EC2实例与数据库软件，并且会知道数据存放的6个存储服务器并清除这些副本中Abort的事务。这时，就需要进行QR Read，因为有些log index可能需要被丢弃。</p>
<p>​   Aurora的读写：写操作时不会直接写，而是会维护每个page的Log list，新的写请求到来时会将Log append到对应的log list中。直到下次需要读这个page的时候，这个page的log list中的log才会apply。</p>
<p>​   Protection Group:由于数据库所需要持久化的数据量超过单台机器的存储空间。因此，需要将这些数据划分成不同的PG，每个PG包含6个存储服务器作为副本。按照data page对PG进行划分，每个PG存储部分data page以及这些data page相关联的log。</p>
<p>​   重建副本：当一个存储服务器挂了之后，假设上面存储了M个PG。我们需要重新找到M个存储服务器，并将这M个PG的副本分别拷贝到这M个存储服务器上。</p>
<p>​   Read-only database:主数据库实例的副本，只提供读请求查询，为主数据库实例分担读请求压力。</p>
<hr />
<h2 id="frangipani">Frangipani</h2>
<p>以下内容很多引用了<a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-11-cache-consistency-frangipani/11.7-gu-zhang-hui-fu-crash-recovery">mit6.824翻译</a></p>
<p>背景：需要实现这样一个系统，有一个中心化的存储中心Petal，每个工作站通过Frangipani可以共享访问Petal的数据，同时有性能要求，即需要在工作站中做大量的缓存（当某文件只有一个工作站在写，而不被其他工作站读写时，会在正在写的工作站本地进行）。</p>
<ul>
<li>这个系统的第一个挑战在于用户期待系统实现缓存一致性的属性，即如果用户A缓存了一个数据，并且用户B在用户A的缓存中修改了这个数据，那么用户A的缓存需要自动的应用用户B的修改。</li>
<li>第二个挑战是原子性的挑战，即当两个用户在同一目录下进行创建文件时，是否能够同时创建成功。</li>
<li>第三个挑战在于故障恢复，当本地工作站进行了大量的修改，在缓存写回Petal之前工作站崩溃了该怎么办。</li>
</ul>
<p>Lock Server:在Petal上存储的每个文件都有锁（共享锁与排他锁）。这些锁由锁服务器进行管理，锁服务器会记住每个文件的锁被谁所持有。在每个工作站中，Frangipani模块也会有一个lock表单，表单会记录文件名、对应的锁的状态和文件的缓存内容。这里的文件内容可能是大量的数据块，也可能是目录的列表。当一个工作站需要读取文件时，他会先向锁服务器请求锁，之后才会向Petal请求数据，然后记录锁的状态和文件内容等。每个锁都有两个状态，busy or idle。当文件正在被增删改时，可以认为是busy的状态，当完成了这些操作后，回随着相应的系统调用释放锁，而变为Idle状态(工作站仍然持有锁，只是没有再使用。延迟还锁，因为用户在创建一个文件之后很大可能会对该文件进行修改或读取)。Frangipani应用锁的规则包括：工作站必须先获取到文件的锁才能从Petal读取数据。工作站也必须先把修改数据写回Petal才能释放锁。</p>
<p>Cache Coherence:系统中有四种不同的RPC。</p>
<ul>
<li>Request:工作站向锁服务器请求锁</li>
<li>Grant：锁服务器同意将锁给工作站</li>
<li>Revoke：给持有锁的工作站，请求释放锁。如果锁为Idle，工作站会将脏数据写回Petal中。</li>
<li>Release：当脏数据写回Petal之后，工作站向锁服务器还锁。</li>
</ul>
<p>​   它确保了直到所有有可能私底下在缓存中修改了数据的工作站先将数据写回到Petal，其他工作站才能读取相应的文件。所以，这里的锁机制确保了读文件总是能看到最新写入文件的数据。</p>
<p>Atomicity:确保我们在整个操作的过程中持有所有的锁，我们就可以无成本的获得这里的不可分割原子事务。</p>
<p>Frangipani Log:当一个工作站需要完成涉及到多个数据的复杂操作时，在工作站向Petal写入任何数据之前，工作站会在Petal中自己的Log列表中追加一个Log条目，这个Log条目会描述整个的需要完成的操作。只有当这个描述了完整操作的Log条目安全的存在于Petal之后，工作站才会开始向Petal发送数据。但是Frangipani在实现WAL时，有一些不同的地方。首先Frangipani对于每个工作站都保存了一份独立的Log。其次，Frangipani的工作站将自己的Log保存在作为共享存储的Petal中。Log包含的信息：LSN（Log序号，因为Log是环形存储，以保证每个工作站的Log占有固定空间，因此需要知道哪里是结尾），写入的块号，版本号与数据（文件的元数据而不是写入的数据，所以恢复出来是空文件？）。</p>
<p>故障恢复：</p>
<ul>
<li>主要考虑当Log写入Petal，但是缓存数据没有写入的情况。会由一个新的工作站WS2会检查Log条目的更新内容，并向Petal执行Log条目中的更新内容。当WS2执行完WS1存放在Petal中的Log，它会通知锁服务器，之后锁服务器会释放WS1持有的锁。另一个有趣的可能是，工作站WS1在写入Log之后，并且在写入块数据的过程中崩溃了。先不考虑一些极其重要的细节，执行恢复的工作站WS2并不知道WS1在哪个位置崩溃的，它只能看到一些Log条目，同样的，WS2会以相同的方式重新执行Log。尽管部分修改已经写入了Petal，WS2会重新执行修改。对于部分已经写入的数据，相当于在相同的位置写入相同的数据。对于部分未写入的数据，相当于更新了Petal中的这部分数据，并完成了操作。</li>
<li>更复杂的场景时，释放锁时崩溃。这样可能会导致一些提前的事务，延迟执行了而带来了错误的结果。Frangipani是通过版本号来解决这一问题的。</li>
<li>还有一个问题在于，恢复过程中可能其他工作站还在频繁的读取文件系统占用需要的锁。（这个问题其实不是问题，恢复过程中不需要关心锁的问题。）</li>
</ul>
<hr />
<h2 id="distributed-transaction">Distributed Transaction</h2>
<p>并发控制：</p>
<ul>
<li>悲观锁并发控制：二阶段锁</li>
<li>OCC</li>
</ul>
<p>原子提交：二阶段提交</p>
<p><center>
  <img alt="2PC" src="../../img/2pc.png" />
    <div>2PC</div>
</center></p>
<p>B crash before Yes:B可以终止事务</p>
<p>B crash after Yes:B在回复Yes之前需要将该事务相关Log持久化到磁盘上，因此当B恢复后可以继续完成它那部分的工作。</p>
<p>B收到两条commit:简单回复ACK即可。</p>
<p>TC crash before commit: TC可以终止事务</p>
<p>TC crash after commit: TC发送commit消息之前也需要持久化事务相关数据。</p>
<p>网络问题：TC长时间没有收到Yes/No / 服务器长时间没有收到prepare后可以单方面终止事务。如果服务器长时间没有收到commit消息，需要长时间一直等待，因为其他服务器可能已经commit了。</p>
<hr />
<h2 id="spanner">Spanner</h2>
<p>背景：Spanner是一个实现数据分片并且每个分片有多个数据副本的系统，主要的挑战包括：想要读取本地服务器的数据（即想就近读follower的数据）以及实现分布式事务功能。</p>
<p><center>
  <img alt="spanner_transcation" src="../../img/spanner_transcation.png" />
    <div>Spanner R/W Transcation</div>
</center></p>
<p>TC复制：两阶段提交由于TC宕机或者网络故障而无法回复commit消息时，会发生阻塞。Spanner通过对TC进行复制，解决了这个问题。</p>
<p>只读事务的线性一致性保证：快照隔离。对于R/W事务时间戳为提交时间，对于R/O事务时间戳为开始时间，所有的事务按照时间戳顺序执行。（<strong>思考</strong>：旧快照垃圾回收）</p>
<p>安全时间：直接读follower数据可能会读到老数据。因此，只能读取安全事件内follower的数据，即只有当follower有比当前R/O事务更新的时间戳的日志时才能进行读取。（Spanner的每个Paxos组的Leader会按log的时间戳顺序同步数据）</p>
<p>真实时间：快照隔离存在时间同步问题，即每个服务器上达到完美的时间同步是不可能的。每个事务获取的时间为一个区间[earliest time, latest time]。</p>
<ul>
<li>事务选择的时间戳为latest time。</li>
<li>同时还需要保证R/W事务提交的时间戳小于下一个读事务的earliest time才能提交。</li>
</ul>
<hr />
<h2 id="farm">FaRM</h2>
<p>NVRAM：非易失性RAM,省去了持久化到磁盘的时间。主要是通过加了一个电源，当供电故障时，FaRM才会将RAM上的数据通过小电源的电量去持久化到磁盘上。（只对供电故障有效）</p>
<p>kernel bypass:应用程序跳过内核直接对内存和网络接口卡进行操作。</p>
<p>RDMA:远程内存访问，可以直接对另一个服务器的内存数据进行修改。（FaRM通过RDMA进行数据库读取，而没有用来写入）</p>
<p>OCC:读取数据不会上锁，写数据先写入buffer中。commit之前需要validation，如果验证成功则commit，否则就abort。</p>
<p>commit：</p>
<p><center>
  <img alt="FaRM_commit" src="../../img/FaRM_commit.png" />
    <div>FaRM commit</div>
</center></p>
<p>Lock阶段：写数据需要拿锁并验证版本号，失败则终止</p>
<p>验证阶段：验证读取对象没有更新（锁和版本号），失败则终止</p>
<hr />
<h2 id="spark">Spark</h2>
<p>背景：Mapreduce对于反复利用中间变量的应用效率底下，会产生大量的I/O。本文提出了RDDs(弹性分布式数据集),用户可以将中间结果显示地持久保存在内存中，控制其分区以优化数据放置，并使用丰富的运算符对其操作。RDDs提供对数据的粗粒度转换（即多很多数据进行同一操作）的接口，并记录这些转换以提供容错能力。</p>
<p>Spark无法处理的情况：在线/流数据（Spark是假定操作和RDD都是固定的）</p>
<hr />
<p>有机会再补充后续吧...</p></p>
    </article>
</section>
            <!-- endblock -->

            <!-- block footer -->
                <footer>
    <div class="d-flex flex-sm-row justify-content-between py-2 border-top drac-text-black drac-bg-cyan-green">
        <a href="https://github.com/dracula/mkdocs" target="_blank" style="padding-left: 1%;"
            class="footer-text drac-anchor drac-text-black drac-text-purple--hover">
            Made with Dracula Theme for MkDocs
        </a>
    </div>
</footer>
            <!-- endblock -->
        </div>

    </main>

        <script>var base_url = '../..';</script>
        <script src="../../assets/js/jquery-3.3.1.slim.min.js""></script>
        <script src="../../assets/js/bootstrap.bundle.min.js""></script>
        <script src="../../assets/js/mkdocs.js""></script>
			<script src="../../search/main.js" defer></script>

</body>

</html>
